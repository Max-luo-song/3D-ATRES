Introduction
3D Referring Expression Segmentation (3D-RES) is an emerging yet challenging task at the interaction of vision and language, which aims to precisely segment a target instance within a 3D point cloud based on a given natural language referring expression. However, most previous methods overlook multi-source ambiguities that are prevalent in real-world scenarios, including prompt, spatial, and annotation ambiguities. Prompt ambiguity arises from confusion between referent and target objects due to ambiguous language, spatial ambiguity results from viewpoint variations causing incomplete segmentation, annotation ambiguity stems from inconsistent or noisy labeling in training data. In this paper, we propose a novel 3D Ambiguity-Tolerant Referring Expression Segmentation (3D-ATRES), which explicitly models and mitigates multi-source ambiguities in 3D-RES. Specifically, we employ $TR^2$ Semantic Structurizer to transform free-form natural language into structured Target–Relation–Referent triples, thereby eliminating referential ambiguity. For spatial ambiguity, we introduce a Normal‑Aware Spatial Alignment that leverages surface normal cues to achieve viewpoint-consistent geometry alignment. To combat annotation ambiguity, we incorporate a Annotation Ambiguity Penalty, enabling the network to learn from noisy or inconsistent annotations in a probabilistic manner. Experiments on ScanRefer and Multi3DRefer show that 3D-ATRES achieves state-of-the-art performance, confirming the effectiveness of modeling ambiguity in 3D-RES. Code will be released upon acceptance.
